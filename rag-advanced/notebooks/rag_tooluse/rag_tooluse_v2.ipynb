{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic RAG with Tool Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tool use allows for greater flexibility in accessing and utilizing data sources, thus unlocking new use cases not possible with a standard RAG approach.\n",
    "\n",
    "In an enterprise setting where data sources are diverse with non-homogeneous formats (structured/semi-structured/unstructured), this approach becomes even more important.\n",
    "\n",
    "In this notebook, we'll look at how we can implement an agentic RAG system using a tool use approach. We'll do this by building a Weights & Biases assistant. The assistant can search for information about how to use the product, retrieve information from the internet, search code examples, and even perform data analysis.\n",
    "\n",
    "Concretely, we'll cover the following use cases:\n",
    "1. Tool routing\n",
    "2. Parallel tool use\n",
    "3. Multi-step tool use\n",
    "4. Self-correction\n",
    "5. Structured queries\n",
    "6. Structured data queries\n",
    "\n",
    "We'll give the assistant access to the following tools:\n",
    "- `search_developer_docs`: Searches the Weights & Biases developer documentation\n",
    "- `search_internet`: Searches the internet for general queries\n",
    "- `search_code_examples`: Searches code examples and tutorials on using Weights & Biases\n",
    "- `analyze_evaluation_results`: Analyzes a table containing results from evaluating an LLM application\n",
    "\n",
    "Note that for simplicity, we are not implementing a full-fledge search. Instead, we'll use a mock datasets containing small, pre-defined data for each tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "To get started, first we need to install the `cohere` library and create a Cohere client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cohere\n",
    "\n",
    "from tool_def_v2 import (\n",
    "    search_developer_docs,\n",
    "    search_internet,\n",
    "    search_code_examples,\n",
    "    analyze_evaluation_results,\n",
    "    search_tools,\n",
    "    analysis_tool\n",
    ")\n",
    "\n",
    "co = cohere.ClientV2(api_key=os.environ[\"COHERE_API_KEY\"]) # Get your free API key: https://dashboard.cohere.com/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U cohere pandas -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an agentic RAG system, each data source is represented as a \"tool\". A tool is broadly any function or service that can receive and send objects to the LLM. But in the case of RAG, this becomes a more specific case of a tool that takes a query as input and return a set of documents.\n",
    "\n",
    "Here, we are defining a Python function for each tool, but more broadly, the tool can be any function or service that can receive and send objects. \n",
    "\n",
    "Note: refer to the `tool_def_v2.py` file for the implementation of each tool.\n",
    "\n",
    "These functions are mapped to a dictionary called functions_map for easy access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_map = {\n",
    "    \"search_developer_docs\": search_developer_docs,\n",
    "    \"search_internet\": search_internet,\n",
    "    \"search_code_examples\": search_code_examples,\n",
    "    \"analyze_evaluation_results\": analyze_evaluation_results\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running an agentic RAG workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run an agentic RAG workflow using a tool use approach. We can think of the system as consisting of four components:\n",
    "- The user\n",
    "- The application\n",
    "- The LLM\n",
    "- The tools\n",
    "\n",
    "At its most basic, these four components interact in a workflow through four steps:\n",
    "- **Step 1: Get user message** – The LLM gets the user message (via the application)\n",
    "- **Step 2: Tool planning and calling** – The LLM makes a decision on the tools to call (if any) and generates - the tool calls\n",
    "- **Step 3: Tool execution** - The application executes the tools and the results are sent to the LLM\n",
    "- **Step 4: Response and citation generation** – The LLM generates the response and citations to back to the user\n",
    "\n",
    "We wrap all these steps in a function called `run_agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message=\"\"\"## Task and Context\n",
    "You are an assistant who helps developers use Weights & Biases. The company is also referred to as Wandb or W&B for short. You are equipped with a number of tools that can provide different types of information. If you can't find the information you need from one tool, you should try other tools if there is a possibility that they could provide the information you need. Use the internet to search for information not available in the sources provided by Weights & Biases\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"command-r-08-2024\"\n",
    "\n",
    "def run_agent(query, tools, messages=None):\n",
    "    if messages is None:\n",
    "        messages = []\n",
    "        \n",
    "    if \"system\" not in {m.get(\"role\") for m in messages}:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    \n",
    "    # Step 1: get user message\n",
    "    print(f\"Question:\\n{query}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    # Step 2: Generate tool calls (if any)\n",
    "    response = co.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    while response.message.tool_calls:\n",
    "        \n",
    "        print(\"Tool plan:\")\n",
    "        print(response.message.tool_plan,\"\\n\")\n",
    "        print(\"Tool calls:\")\n",
    "        for tc in response.message.tool_calls:\n",
    "            if tc.function.name == \"analyze_evaluation_results\":\n",
    "                print(f\"Tool name: {tc.function.name}\")\n",
    "                tool_call_prettified = print(\"\\n\".join(f\"  {line}\" for line_num, line in enumerate(json.loads(tc.function.arguments)[\"code\"].splitlines())))\n",
    "                print(tool_call_prettified)\n",
    "            else:\n",
    "                print(f\"Tool name: {tc.function.name} | Parameters: {tc.function.arguments}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"tool_calls\": response.message.tool_calls, \"tool_plan\": response.message.tool_plan})        \n",
    "        \n",
    "        # Step 3: Get tool results\n",
    "        tool_content = []\n",
    "        for idx, tc in enumerate(response.message.tool_calls):\n",
    "            tool_result = functions_map[tc.function.name](**json.loads(tc.function.arguments))\n",
    "            tool_content.append(json.dumps(tool_result))\n",
    "            messages.append({\"role\": \"tool\", \"tool_call_id\": tc.id, \"content\": tool_content})\n",
    "        \n",
    "        # Step 4: Generate response and citations \n",
    "        response = co.chat(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            temperature=0.1\n",
    "        )\n",
    "    \n",
    "    messages.append({\"role\": \"assistant\", \"content\": response.message.content[0].text})\n",
    "        \n",
    "    # Print final response\n",
    "    print(\"Response:\")\n",
    "    print(response.message.content[0].text)\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Print citations (if any)\n",
    "    if response.message.citations:\n",
    "        print(\"\\nCITATIONS:\")\n",
    "        for citation in response.message.citations:\n",
    "            print(citation, \"\\n\")\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Tool routing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With tool routing, the agent decides which tool(s) to use based on the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Where can I find the output of a run\n",
      "==================================================\n",
      "Tool plan:\n",
      "I will search for 'where to find output of a run' in the developer documentation. \n",
      "\n",
      "Tool calls:\n",
      "Tool name: search_developer_docs | Parameters: {\"query\":\"where to find output of a run\"}\n",
      "==================================================\n",
      "Response:\n",
      "To view a run, navigate to the W&B App UI, select the relevant project, and then choose the run from the 'Runs' table.\n",
      "==================================================\n",
      "\n",
      "CITATIONS:\n",
      "start=15 end=118 text=\"navigate to the W&B App UI, select the relevant project, and then choose the run from the 'Runs' table.\" sources=[ToolSource(type='tool', id='search_developer_docs_cncf7bghy0x8:0', tool_output={'developer_docs': '[{\"text\":\"What is a W\\\\u0026B Run?\\\\nA W\\\\u0026B run is a single unit of computation logged by W\\\\u0026B, representing an atomic element of your project.\"},{\"text\":\"How do I view a specific run?\\\\nTo view a run, navigate to the W\\\\u0026B App UI, select the relevant project, and then choose the run from the \\'Runs\\' table.\"},{\"text\":\"What are Artifact Outputs?\\\\nArtifact Outputs refer to any artifacts produced by a run.\"},{\"text\":\"How can I do hyperparameter search quickly?\\\\nUse W\\\\u0026B Sweeps to automate hyperparameter search and visualize rich, interactive experiment tracking.\"}]'})] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"Where can I find the output of a run\", search_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Who are the authors of the sentence BERT paper?\n",
      "==================================================\n",
      "Tool plan:\n",
      "I will search for the authors of the sentence BERT paper. \n",
      "\n",
      "Tool calls:\n",
      "Tool name: search_internet | Parameters: {\"query\":\"authors of the sentence BERT paper\"}\n",
      "==================================================\n",
      "Response:\n",
      "The authors of the sentence BERT paper are Nils Reimers and Iryna Gurevych.\n",
      "==================================================\n",
      "\n",
      "CITATIONS:\n",
      "start=43 end=47 text='Nils' sources=[ToolSource(type='tool', id='search_internet_c9s1dt82d6mx:0', tool_output={'documents': '[{\"content\":\"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks - ACL Anthology In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982–3992, Hong Kong, China. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (Reimers \\\\u0026 Gurevych, EMNLP-IJCNLP 2019) abstract = \\\\\"BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://aclanthology.org/D19-1410) (Reimers \\\\u0026 Gurevych, EMNLP-IJCNLP 2019) Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (Reimers \\\\u0026 Gurevych, EMNLP-IJCNLP 2019) In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982–3992, Hong Kong, China.\",\"url\":\"https://aclanthology.org/D19-1410/\"},{\"content\":\"In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. We fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent Conneau et\\xa0al. The paper is structured in the following way: Section 3 presents SBERT, section 4 evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity (AFS) corpus Misra et\\xa0al. To bypass this limitations, researchers passed single sentences through BERT and then derive a fixed sized vector by either averaging the outputs (similar to average word embeddings) or by using the output of the special CLS token (for example: May et\\xa0al.\",\"url\":\"https://ar5iv.labs.arxiv.org/html/1908.10084\"},{\"content\":\"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks Nils Reimers and Iryna Gurevych Ubiquitous Knowledge Processing Lab (UKP-TUDA) Department of Computer Science, Technische Universit¨at Darmstadt www.ukp.tu-darmstadt.de Abstract BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). GloVe embeddings 55.14 70.66 59.73 68.25 63.66 58.02 53.76 61.32 Avg. BERT embeddings 38.78 57.98 57.98 63.15 61.06 46.35 58.40 54.81 BERT CLS-vector 20.16 30.01 20.09 36.88 38.08 16.50 42.63 29.19 InferSent - Glove 52.86 66.75 62.15 72.77 66.87 68.03 65.65 65.01 Universal Sentence Encoder 64.49 67.80 64.61 76.83 73.18 74.92 76.69 71.22 SBERT-NLI-base 70.97 76.53 73.19 79.09 74.30 77.03 72.91 74.89 SBERT-NLI-large 72.27 78.46 74.90 80.99 76.25 79.23 73.75 76.55 SRoBERTa-NLI-base 71.54 72.49 70.80 78.74 73.69 77.77 74.46 74.21 SRoBERTa-NLI-large 74.53 77.00 73.18 81.85 76.82 79.10 74.29 76.68 Table 1: Spearman rank correlation ρ between the cosine similarity of sentence representations and the gold labels for various Textual Similarity (STS) tasks.\",\"url\":\"https://arxiv.org/pdf/1908.10084\"},{\"content\":\"In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. SBERT: modification of the BERT network using siamese and triplet networks that is able to derive semantically meaningful sentence embeddings. SBERT: use pre-trained BERT and RoBERTa network and only fine-tune it to yield useful sentence embeddings In order to fine-tune BERT / RoBERTa, we create siamese and triplet networks (Schroff et al., 2015) to update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity. Regression Objective Function: cosine-similarity between two sentence embeddings $u$ and $v$ is computed.\",\"url\":\"https://sybock.github.io/sbert/\"},{\"content\":\"[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/linear-probe-classification-on-senteval)](https://paperswithcode.com/sota/linear-probe-classification-on-senteval?p=sentence-bert-sentence-embeddings-using) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/semantic-textual-similarity-on-sick)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sick?p=sentence-bert-sentence-embeddings-using) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/semantic-textual-similarity-on-sts12)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts12?p=sentence-bert-sentence-embeddings-using) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/semantic-textual-similarity-on-sts14)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts14?p=sentence-bert-sentence-embeddings-using) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/semantic-textual-similarity-on-sts15)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts15?p=sentence-bert-sentence-embeddings-using) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/semantic-textual-similarity-on-sts16)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts16?p=sentence-bert-sentence-embeddings-using) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/semantic-textual-similarity-on-sts13)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts13?p=sentence-bert-sentence-embeddings-using) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/semantic-textual-similarity-on-sts-benchmark)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts-benchmark?p=sentence-bert-sentence-embeddings-using) BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods. Clustering Linear-Probe Classification Semantic Similarity Semantic Textual Similarity Sentence Sentence Embedding Sentence Embeddings STS Transfer Learning Adam • Attention Dropout • BERT • Dense Connections • Dropout • GELU • Layer Normalization • Linear Layer • Linear Warmup With Linear Decay • Multi-Head Attention • Residual Connection • RoBERTa • SBERT • Scaled Dot-Product Attention • Softmax • Weight Decay • WordPiece\",\"url\":\"https://paperswithcode.com/paper/sentence-bert-sentence-embeddings-using\"}]'})] \n",
      "\n",
      "start=48 end=55 text='Reimers' sources=[ToolSource(type='tool', id='search_internet_c9s1dt82d6mx:0', tool_output={'documents': '[{\"content\":\"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks - ACL Anthology In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982–3992, Hong Kong, China. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (Reimers \\\\u0026 Gurevych, EMNLP-IJCNLP 2019) abstract = \\\\\"BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://aclanthology.org/D19-1410) (Reimers \\\\u0026 Gurevych, EMNLP-IJCNLP 2019) Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (Reimers \\\\u0026 Gurevych, EMNLP-IJCNLP 2019) In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982–3992, Hong Kong, China.\",\"url\":\"https://aclanthology.org/D19-1410/\"},{\"content\":\"In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. We fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent Conneau et\\xa0al. The paper is structured in the following way: Section 3 presents SBERT, section 4 evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity (AFS) corpus Misra et\\xa0al. To bypass this limitations, researchers passed single sentences through BERT and then derive a fixed sized vector by either averaging the outputs (similar to average word embeddings) or by using the output of the special CLS token (for example: May et\\xa0al.\",\"url\":\"https://ar5iv.labs.arxiv.org/html/1908.10084\"},{\"content\":\"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks Nils Reimers and Iryna Gurevych Ubiquitous Knowledge Processing Lab (UKP-TUDA) Department of Computer Science, Technische Universit¨at Darmstadt www.ukp.tu-darmstadt.de Abstract BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). GloVe embeddings 55.14 70.66 59.73 68.25 63.66 58.02 53.76 61.32 Avg. BERT embeddings 38.78 57.98 57.98 63.15 61.06 46.35 58.40 54.81 BERT CLS-vector 20.16 30.01 20.09 36.88 38.08 16.50 42.63 29.19 InferSent - Glove 52.86 66.75 62.15 72.77 66.87 68.03 65.65 65.01 Universal Sentence Encoder 64.49 67.80 64.61 76.83 73.18 74.92 76.69 71.22 SBERT-NLI-base 70.97 76.53 73.19 79.09 74.30 77.03 72.91 74.89 SBERT-NLI-large 72.27 78.46 74.90 80.99 76.25 79.23 73.75 76.55 SRoBERTa-NLI-base 71.54 72.49 70.80 78.74 73.69 77.77 74.46 74.21 SRoBERTa-NLI-large 74.53 77.00 73.18 81.85 76.82 79.10 74.29 76.68 Table 1: Spearman rank correlation ρ between the cosine similarity of sentence representations and the gold labels for various Textual Similarity (STS) tasks.\",\"url\":\"https://arxiv.org/pdf/1908.10084\"},{\"content\":\"In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. SBERT: modification of the BERT network using siamese and triplet networks that is able to derive semantically meaningful sentence embeddings. SBERT: use pre-trained BERT and RoBERTa network and only fine-tune it to yield useful sentence embeddings In order to fine-tune BERT / RoBERTa, we create siamese and triplet networks (Schroff et al., 2015) to update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity. Regression Objective Function: cosine-similarity between two sentence embeddings $u$ and $v$ is computed.\",\"url\":\"https://sybock.github.io/sbert/\"},{\"content\":\"[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/linear-probe-classification-on-senteval)](https://paperswithcode.com/sota/linear-probe-classification-on-senteval?p=sentence-bert-sentence-embeddings-using) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/semantic-textual-similarity-on-sick)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sick?p=sentence-bert-sentence-embeddings-using) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/semantic-textual-similarity-on-sts12)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts12?p=sentence-bert-sentence-embeddings-using) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/semantic-textual-similarity-on-sts14)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts14?p=sentence-bert-sentence-embeddings-using) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/semantic-textual-similarity-on-sts15)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts15?p=sentence-bert-sentence-embeddings-using) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/semantic-textual-similarity-on-sts16)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts16?p=sentence-bert-sentence-embeddings-using) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/semantic-textual-similarity-on-sts13)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts13?p=sentence-bert-sentence-embeddings-using) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/semantic-textual-similarity-on-sts-benchmark)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts-benchmark?p=sentence-bert-sentence-embeddings-using) BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods. Clustering Linear-Probe Classification Semantic Similarity Semantic Textual Similarity Sentence Sentence Embedding Sentence Embeddings STS Transfer Learning Adam • Attention Dropout • BERT • Dense Connections • Dropout • GELU • Layer Normalization • Linear Layer • Linear Warmup With Linear Decay • Multi-Head Attention • Residual Connection • RoBERTa • SBERT • Scaled Dot-Product Attention • Softmax • Weight Decay • WordPiece\",\"url\":\"https://paperswithcode.com/paper/sentence-bert-sentence-embeddings-using\"}]'})] \n",
      "\n",
      "start=60 end=75 text='Iryna Gurevych.' sources=[ToolSource(type='tool', id='search_internet_c9s1dt82d6mx:0', tool_output={'documents': '[{\"content\":\"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks - ACL Anthology In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982–3992, Hong Kong, China. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (Reimers \\\\u0026 Gurevych, EMNLP-IJCNLP 2019) abstract = \\\\\"BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://aclanthology.org/D19-1410) (Reimers \\\\u0026 Gurevych, EMNLP-IJCNLP 2019) Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (Reimers \\\\u0026 Gurevych, EMNLP-IJCNLP 2019) In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982–3992, Hong Kong, China.\",\"url\":\"https://aclanthology.org/D19-1410/\"},{\"content\":\"In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. We fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent Conneau et\\xa0al. The paper is structured in the following way: Section 3 presents SBERT, section 4 evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity (AFS) corpus Misra et\\xa0al. To bypass this limitations, researchers passed single sentences through BERT and then derive a fixed sized vector by either averaging the outputs (similar to average word embeddings) or by using the output of the special CLS token (for example: May et\\xa0al.\",\"url\":\"https://ar5iv.labs.arxiv.org/html/1908.10084\"},{\"content\":\"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks Nils Reimers and Iryna Gurevych Ubiquitous Knowledge Processing Lab (UKP-TUDA) Department of Computer Science, Technische Universit¨at Darmstadt www.ukp.tu-darmstadt.de Abstract BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). GloVe embeddings 55.14 70.66 59.73 68.25 63.66 58.02 53.76 61.32 Avg. BERT embeddings 38.78 57.98 57.98 63.15 61.06 46.35 58.40 54.81 BERT CLS-vector 20.16 30.01 20.09 36.88 38.08 16.50 42.63 29.19 InferSent - Glove 52.86 66.75 62.15 72.77 66.87 68.03 65.65 65.01 Universal Sentence Encoder 64.49 67.80 64.61 76.83 73.18 74.92 76.69 71.22 SBERT-NLI-base 70.97 76.53 73.19 79.09 74.30 77.03 72.91 74.89 SBERT-NLI-large 72.27 78.46 74.90 80.99 76.25 79.23 73.75 76.55 SRoBERTa-NLI-base 71.54 72.49 70.80 78.74 73.69 77.77 74.46 74.21 SRoBERTa-NLI-large 74.53 77.00 73.18 81.85 76.82 79.10 74.29 76.68 Table 1: Spearman rank correlation ρ between the cosine similarity of sentence representations and the gold labels for various Textual Similarity (STS) tasks.\",\"url\":\"https://arxiv.org/pdf/1908.10084\"},{\"content\":\"In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. SBERT: modification of the BERT network using siamese and triplet networks that is able to derive semantically meaningful sentence embeddings. SBERT: use pre-trained BERT and RoBERTa network and only fine-tune it to yield useful sentence embeddings In order to fine-tune BERT / RoBERTa, we create siamese and triplet networks (Schroff et al., 2015) to update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity. Regression Objective Function: cosine-similarity between two sentence embeddings $u$ and $v$ is computed.\",\"url\":\"https://sybock.github.io/sbert/\"},{\"content\":\"[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/linear-probe-classification-on-senteval)](https://paperswithcode.com/sota/linear-probe-classification-on-senteval?p=sentence-bert-sentence-embeddings-using) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/semantic-textual-similarity-on-sick)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sick?p=sentence-bert-sentence-embeddings-using) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/semantic-textual-similarity-on-sts12)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts12?p=sentence-bert-sentence-embeddings-using) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/semantic-textual-similarity-on-sts14)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts14?p=sentence-bert-sentence-embeddings-using) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/semantic-textual-similarity-on-sts15)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts15?p=sentence-bert-sentence-embeddings-using) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/semantic-textual-similarity-on-sts16)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts16?p=sentence-bert-sentence-embeddings-using) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/semantic-textual-similarity-on-sts13)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts13?p=sentence-bert-sentence-embeddings-using) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/sentence-bert-sentence-embeddings-using/semantic-textual-similarity-on-sts-benchmark)](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts-benchmark?p=sentence-bert-sentence-embeddings-using) BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods. Clustering Linear-Probe Classification Semantic Similarity Semantic Textual Similarity Sentence Sentence Embedding Sentence Embeddings STS Transfer Learning Adam • Attention Dropout • BERT • Dense Connections • Dropout • GELU • Layer Normalization • Linear Layer • Linear Warmup With Linear Decay • Multi-Head Attention • Residual Connection • RoBERTa • SBERT • Scaled Dot-Product Attention • Softmax • Weight Decay • WordPiece\",\"url\":\"https://paperswithcode.com/paper/sentence-bert-sentence-embeddings-using\"}]'})] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"Who are the authors of the sentence BERT paper?\", search_tools)\n",
    "# chooses search_internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Parallel tool use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent can call multiple tools in parallel. In this example, given that the user is asking about two different things in a single message, the agent generates two parallel tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Explain what is a W&B Run and how do I view a specific run\n",
      "==================================================\n",
      "Tool plan:\n",
      "I will search for 'W&B Run' and 'view a specific run' in the developer documentation. \n",
      "\n",
      "Tool calls:\n",
      "Tool name: search_developer_docs | Parameters: {\"query\":\"W\\u0026B Run\"}\n",
      "Tool name: search_developer_docs | Parameters: {\"query\":\"view a specific run\"}\n",
      "==================================================\n",
      "Response:\n",
      "A W&B run is a single unit of computation logged by W&B, representing an atomic element of your project.\n",
      "\n",
      "To view a specific run, navigate to the W&B App UI, select the relevant project, and then choose the run from the 'Runs' table.\n",
      "==================================================\n",
      "\n",
      "CITATIONS:\n",
      "start=15 end=55 text='single unit of computation logged by W&B' sources=[ToolSource(type='tool', id='search_developer_docs_0gqbme8c2q67:0', tool_output={'developer_docs': '[{\"text\":\"What is a W\\\\u0026B Run?\\\\nA W\\\\u0026B run is a single unit of computation logged by W\\\\u0026B, representing an atomic element of your project.\"},{\"text\":\"How do I view a specific run?\\\\nTo view a run, navigate to the W\\\\u0026B App UI, select the relevant project, and then choose the run from the \\'Runs\\' table.\"},{\"text\":\"What are Artifact Outputs?\\\\nArtifact Outputs refer to any artifacts produced by a run.\"},{\"text\":\"How can I do hyperparameter search quickly?\\\\nUse W\\\\u0026B Sweeps to automate hyperparameter search and visualize rich, interactive experiment tracking.\"}]'}), ToolSource(type='tool', id='search_developer_docs_0gqbme8c2q67:1', tool_output={'developer_docs': '[{\"text\":\"What is a W\\\\u0026B Run?\\\\nA W\\\\u0026B run is a single unit of computation logged by W\\\\u0026B, representing an atomic element of your project.\"},{\"text\":\"How do I view a specific run?\\\\nTo view a run, navigate to the W\\\\u0026B App UI, select the relevant project, and then choose the run from the \\'Runs\\' table.\"},{\"text\":\"What are Artifact Outputs?\\\\nArtifact Outputs refer to any artifacts produced by a run.\"},{\"text\":\"How can I do hyperparameter search quickly?\\\\nUse W\\\\u0026B Sweeps to automate hyperparameter search and visualize rich, interactive experiment tracking.\"}]'}), ToolSource(type='tool', id='search_developer_docs_wv3ch730eqnx:0', tool_output={'developer_docs': '[{\"text\":\"What is a W\\\\u0026B Run?\\\\nA W\\\\u0026B run is a single unit of computation logged by W\\\\u0026B, representing an atomic element of your project.\"},{\"text\":\"How do I view a specific run?\\\\nTo view a run, navigate to the W\\\\u0026B App UI, select the relevant project, and then choose the run from the \\'Runs\\' table.\"},{\"text\":\"What are Artifact Outputs?\\\\nArtifact Outputs refer to any artifacts produced by a run.\"},{\"text\":\"How can I do hyperparameter search quickly?\\\\nUse W\\\\u0026B Sweeps to automate hyperparameter search and visualize rich, interactive experiment tracking.\"}]'}), ToolSource(type='tool', id='search_developer_docs_wv3ch730eqnx:1', tool_output={'developer_docs': '[{\"text\":\"What is a W\\\\u0026B Run?\\\\nA W\\\\u0026B run is a single unit of computation logged by W\\\\u0026B, representing an atomic element of your project.\"},{\"text\":\"How do I view a specific run?\\\\nTo view a run, navigate to the W\\\\u0026B App UI, select the relevant project, and then choose the run from the \\'Runs\\' table.\"},{\"text\":\"What are Artifact Outputs?\\\\nArtifact Outputs refer to any artifacts produced by a run.\"},{\"text\":\"How can I do hyperparameter search quickly?\\\\nUse W\\\\u0026B Sweeps to automate hyperparameter search and visualize rich, interactive experiment tracking.\"}]'})] \n",
      "\n",
      "start=57 end=104 text='representing an atomic element of your project.' sources=[ToolSource(type='tool', id='search_developer_docs_0gqbme8c2q67:0', tool_output={'developer_docs': '[{\"text\":\"What is a W\\\\u0026B Run?\\\\nA W\\\\u0026B run is a single unit of computation logged by W\\\\u0026B, representing an atomic element of your project.\"},{\"text\":\"How do I view a specific run?\\\\nTo view a run, navigate to the W\\\\u0026B App UI, select the relevant project, and then choose the run from the \\'Runs\\' table.\"},{\"text\":\"What are Artifact Outputs?\\\\nArtifact Outputs refer to any artifacts produced by a run.\"},{\"text\":\"How can I do hyperparameter search quickly?\\\\nUse W\\\\u0026B Sweeps to automate hyperparameter search and visualize rich, interactive experiment tracking.\"}]'}), ToolSource(type='tool', id='search_developer_docs_0gqbme8c2q67:1', tool_output={'developer_docs': '[{\"text\":\"What is a W\\\\u0026B Run?\\\\nA W\\\\u0026B run is a single unit of computation logged by W\\\\u0026B, representing an atomic element of your project.\"},{\"text\":\"How do I view a specific run?\\\\nTo view a run, navigate to the W\\\\u0026B App UI, select the relevant project, and then choose the run from the \\'Runs\\' table.\"},{\"text\":\"What are Artifact Outputs?\\\\nArtifact Outputs refer to any artifacts produced by a run.\"},{\"text\":\"How can I do hyperparameter search quickly?\\\\nUse W\\\\u0026B Sweeps to automate hyperparameter search and visualize rich, interactive experiment tracking.\"}]'}), ToolSource(type='tool', id='search_developer_docs_wv3ch730eqnx:0', tool_output={'developer_docs': '[{\"text\":\"What is a W\\\\u0026B Run?\\\\nA W\\\\u0026B run is a single unit of computation logged by W\\\\u0026B, representing an atomic element of your project.\"},{\"text\":\"How do I view a specific run?\\\\nTo view a run, navigate to the W\\\\u0026B App UI, select the relevant project, and then choose the run from the \\'Runs\\' table.\"},{\"text\":\"What are Artifact Outputs?\\\\nArtifact Outputs refer to any artifacts produced by a run.\"},{\"text\":\"How can I do hyperparameter search quickly?\\\\nUse W\\\\u0026B Sweeps to automate hyperparameter search and visualize rich, interactive experiment tracking.\"}]'}), ToolSource(type='tool', id='search_developer_docs_wv3ch730eqnx:1', tool_output={'developer_docs': '[{\"text\":\"What is a W\\\\u0026B Run?\\\\nA W\\\\u0026B run is a single unit of computation logged by W\\\\u0026B, representing an atomic element of your project.\"},{\"text\":\"How do I view a specific run?\\\\nTo view a run, navigate to the W\\\\u0026B App UI, select the relevant project, and then choose the run from the \\'Runs\\' table.\"},{\"text\":\"What are Artifact Outputs?\\\\nArtifact Outputs refer to any artifacts produced by a run.\"},{\"text\":\"How can I do hyperparameter search quickly?\\\\nUse W\\\\u0026B Sweeps to automate hyperparameter search and visualize rich, interactive experiment tracking.\"}]'})] \n",
      "\n",
      "start=130 end=233 text=\"navigate to the W&B App UI, select the relevant project, and then choose the run from the 'Runs' table.\" sources=[ToolSource(type='tool', id='search_developer_docs_0gqbme8c2q67:0', tool_output={'developer_docs': '[{\"text\":\"What is a W\\\\u0026B Run?\\\\nA W\\\\u0026B run is a single unit of computation logged by W\\\\u0026B, representing an atomic element of your project.\"},{\"text\":\"How do I view a specific run?\\\\nTo view a run, navigate to the W\\\\u0026B App UI, select the relevant project, and then choose the run from the \\'Runs\\' table.\"},{\"text\":\"What are Artifact Outputs?\\\\nArtifact Outputs refer to any artifacts produced by a run.\"},{\"text\":\"How can I do hyperparameter search quickly?\\\\nUse W\\\\u0026B Sweeps to automate hyperparameter search and visualize rich, interactive experiment tracking.\"}]'}), ToolSource(type='tool', id='search_developer_docs_0gqbme8c2q67:1', tool_output={'developer_docs': '[{\"text\":\"What is a W\\\\u0026B Run?\\\\nA W\\\\u0026B run is a single unit of computation logged by W\\\\u0026B, representing an atomic element of your project.\"},{\"text\":\"How do I view a specific run?\\\\nTo view a run, navigate to the W\\\\u0026B App UI, select the relevant project, and then choose the run from the \\'Runs\\' table.\"},{\"text\":\"What are Artifact Outputs?\\\\nArtifact Outputs refer to any artifacts produced by a run.\"},{\"text\":\"How can I do hyperparameter search quickly?\\\\nUse W\\\\u0026B Sweeps to automate hyperparameter search and visualize rich, interactive experiment tracking.\"}]'}), ToolSource(type='tool', id='search_developer_docs_wv3ch730eqnx:0', tool_output={'developer_docs': '[{\"text\":\"What is a W\\\\u0026B Run?\\\\nA W\\\\u0026B run is a single unit of computation logged by W\\\\u0026B, representing an atomic element of your project.\"},{\"text\":\"How do I view a specific run?\\\\nTo view a run, navigate to the W\\\\u0026B App UI, select the relevant project, and then choose the run from the \\'Runs\\' table.\"},{\"text\":\"What are Artifact Outputs?\\\\nArtifact Outputs refer to any artifacts produced by a run.\"},{\"text\":\"How can I do hyperparameter search quickly?\\\\nUse W\\\\u0026B Sweeps to automate hyperparameter search and visualize rich, interactive experiment tracking.\"}]'}), ToolSource(type='tool', id='search_developer_docs_wv3ch730eqnx:1', tool_output={'developer_docs': '[{\"text\":\"What is a W\\\\u0026B Run?\\\\nA W\\\\u0026B run is a single unit of computation logged by W\\\\u0026B, representing an atomic element of your project.\"},{\"text\":\"How do I view a specific run?\\\\nTo view a run, navigate to the W\\\\u0026B App UI, select the relevant project, and then choose the run from the \\'Runs\\' table.\"},{\"text\":\"What are Artifact Outputs?\\\\nArtifact Outputs refer to any artifacts produced by a run.\"},{\"text\":\"How can I do hyperparameter search quickly?\\\\nUse W\\\\u0026B Sweeps to automate hyperparameter search and visualize rich, interactive experiment tracking.\"}]'})] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"Explain what is a W&B Run and how do I view a specific run\", search_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Multi-step tool use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be scenarios where tool calling needs to happen in a sequence. For example, when the output of one tool call is needed as input for another tool call.\n",
    "\n",
    "In this example, the agent first searches the developer docs for information about how to view a run. Then, it uses the information to search for a code example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What's that feature to automate hyperparameter search? Do you have some code tutorials?\n",
      "==================================================\n",
      "Tool plan:\n",
      "I will search for the feature to automate hyperparameter search. Then, I will search for code tutorials for this feature. \n",
      "\n",
      "Tool calls:\n",
      "Tool name: search_developer_docs | Parameters: {\"query\":\"feature to automate hyperparameter search\"}\n",
      "==================================================\n",
      "Tool plan:\n",
      "I found that the feature to automate hyperparameter search is called W&B Sweeps. I will now search for code tutorials on W&B Sweeps. \n",
      "\n",
      "Tool calls:\n",
      "Tool name: search_code_examples | Parameters: {\"file_type\":\"null\",\"language\":\"null\",\"query\":\"W\\u0026B Sweeps\"}\n",
      "==================================================\n",
      "Response:\n",
      "W&B Sweeps is the feature to automate hyperparameter search. Unfortunately, I was unable to find any code tutorials for this feature.\n",
      "==================================================\n",
      "\n",
      "CITATIONS:\n",
      "start=0 end=10 text='W&B Sweeps' sources=[ToolSource(type='tool', id='search_developer_docs_bgka6480daeb:0', tool_output={'developer_docs': '[{\"text\":\"What is a W\\\\u0026B Run?\\\\nA W\\\\u0026B run is a single unit of computation logged by W\\\\u0026B, representing an atomic element of your project.\"},{\"text\":\"How do I view a specific run?\\\\nTo view a run, navigate to the W\\\\u0026B App UI, select the relevant project, and then choose the run from the \\'Runs\\' table.\"},{\"text\":\"What are Artifact Outputs?\\\\nArtifact Outputs refer to any artifacts produced by a run.\"},{\"text\":\"How can I do hyperparameter search quickly?\\\\nUse W\\\\u0026B Sweeps to automate hyperparameter search and visualize rich, interactive experiment tracking.\"}]'})] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_history = run_agent(\"What's that feature to automate hyperparameter search? Do you have some code tutorials?\", search_tools)\n",
    "# Does two steps of tool use in a sequence\n",
    "# Returns code examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Self-correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of multi-step tool use can be extended to self-correction. Given the output of the current tool call, the agent may decide to change its plan i.e. self-correct. \n",
    "\n",
    "In this example, the agent doesn't find the information it's looking for in the developer docs. Thus, it generates a new tool call to search the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What is Wandb's weave solution?\n",
      "==================================================\n",
      "Tool plan:\n",
      "I will search for 'Wandb's weave solution' in the developer documentation. \n",
      "\n",
      "Tool calls:\n",
      "Tool name: search_developer_docs | Parameters: {\"query\":\"Wandb's weave solution\"}\n",
      "==================================================\n",
      "Tool plan:\n",
      "I couldn't find any information about Wandb's weave solution in the developer documentation. I will now search the internet to see if I can find any information. \n",
      "\n",
      "Tool calls:\n",
      "Tool name: search_internet | Parameters: {\"query\":\"Wandb's weave solution\"}\n",
      "==================================================\n",
      "Response:\n",
      "Weights & Biases Weave is a lightweight toolkit for tracking and evaluating LLM applications. It is designed with the developer experience in mind, providing capabilities for straightforward evaluation and debugging of GenAI applications.\n",
      "\n",
      "You can use Weave to:\n",
      "- Log and debug language model inputs, outputs, and traces\n",
      "- Build rigorous, apples-to-apples evaluations for language model use cases\n",
      "- Organize all the information generated across the LLM workflow, from experimentation to evaluations to production\n",
      "==================================================\n",
      "\n",
      "CITATIONS:\n",
      "start=28 end=93 text='lightweight toolkit for tracking and evaluating LLM applications.' sources=[ToolSource(type='tool', id='search_internet_k07xfwrwwbz1:0', tool_output={'documents': '[{\"content\":\"Introduction | W\\\\u0026B Weave Open App Trace LLMs Trace Applications App versioning Build an Evaluation Evaluate a RAG App LLM Application Tracing Models Evaluations LLM Providers Local Models On this page Weave is a lightweight toolkit for tracking and evaluating LLM applications, built by Weights \\\\u0026 Biases. Get started by decorating Python functions with @weave.op(). You can use Weave to: Log and debug language model inputs, outputs, and traces Build rigorous, apples-to-apples evaluations for language model use cases Organize all the information generated across the LLM workflow, from experimentation to evaluations to production Try the Quickstart to see Weave in action. Edit this page Next Trace LLMs Weave Weave by W\\\\u0026B\",\"url\":\"https://weave-docs.wandb.ai/\"},{\"content\":\"The solution to this challenge is to treat the model as a closed system where only the inputs and outputs are visible and follow a scientific workflow. This experimental workflow is similar to the workflow machine learning practitioners use to build these LLMs in the first place. W\\\\u0026B Weave was built to enable this experimental workflow:\",\"url\":\"https://wandb.ai/site/press-release/weave-announcement\"},{\"content\":\"Registry Publish and share your ML models and datasets Tables Visualize and explore your ML data Develop GenAI applications with confidence using W\\\\u0026B Weave W\\\\u0026B Weave is here to help developers build and iterate on their AI applications with confidence. Compare different evaluations of model results against different dimensions of performance to ensure applications are as robust as possible when deploying to production. Weave automatically captures all input and output data and builds a tree to help you understand how data flows through your application. Weave is designed with the developer experience in mind, providing capabilities for straightforward evaluation and debugging of GenAI applications. Track and visualize your ML experiments Publish and share your ML models and datasets ML model registry ML model registry\",\"url\":\"https://wandb.ai/site/weave/\"},{\"content\":\"Tutorial: Build an Evaluation pipeline To iterate on an application, we need a way to evaluate if it\\'s improving. To do so, a common practice is to test it against the same set of examples when there is a change. Weave has a first-class way to track evaluations with Model \\\\u0026 Evaluation classes. We have built the APIs to make minimal assumptions to allow for the flexibility to support a wide ...\",\"url\":\"https://weave-docs.wandb.ai/tutorial-eval/\"},{\"content\":\"Weave by Weights \\\\u0026 Biases Weave is a toolkit for developing Generative AI applications, built by Weights \\\\u0026 Biases. You can use Weave to:\",\"url\":\"https://github.com/wandb/weave\"}]'})] \n",
      "\n",
      "start=100 end=146 text='designed with the developer experience in mind' sources=[ToolSource(type='tool', id='search_internet_k07xfwrwwbz1:0', tool_output={'documents': '[{\"content\":\"Introduction | W\\\\u0026B Weave Open App Trace LLMs Trace Applications App versioning Build an Evaluation Evaluate a RAG App LLM Application Tracing Models Evaluations LLM Providers Local Models On this page Weave is a lightweight toolkit for tracking and evaluating LLM applications, built by Weights \\\\u0026 Biases. Get started by decorating Python functions with @weave.op(). You can use Weave to: Log and debug language model inputs, outputs, and traces Build rigorous, apples-to-apples evaluations for language model use cases Organize all the information generated across the LLM workflow, from experimentation to evaluations to production Try the Quickstart to see Weave in action. Edit this page Next Trace LLMs Weave Weave by W\\\\u0026B\",\"url\":\"https://weave-docs.wandb.ai/\"},{\"content\":\"The solution to this challenge is to treat the model as a closed system where only the inputs and outputs are visible and follow a scientific workflow. This experimental workflow is similar to the workflow machine learning practitioners use to build these LLMs in the first place. W\\\\u0026B Weave was built to enable this experimental workflow:\",\"url\":\"https://wandb.ai/site/press-release/weave-announcement\"},{\"content\":\"Registry Publish and share your ML models and datasets Tables Visualize and explore your ML data Develop GenAI applications with confidence using W\\\\u0026B Weave W\\\\u0026B Weave is here to help developers build and iterate on their AI applications with confidence. Compare different evaluations of model results against different dimensions of performance to ensure applications are as robust as possible when deploying to production. Weave automatically captures all input and output data and builds a tree to help you understand how data flows through your application. Weave is designed with the developer experience in mind, providing capabilities for straightforward evaluation and debugging of GenAI applications. Track and visualize your ML experiments Publish and share your ML models and datasets ML model registry ML model registry\",\"url\":\"https://wandb.ai/site/weave/\"},{\"content\":\"Tutorial: Build an Evaluation pipeline To iterate on an application, we need a way to evaluate if it\\'s improving. To do so, a common practice is to test it against the same set of examples when there is a change. Weave has a first-class way to track evaluations with Model \\\\u0026 Evaluation classes. We have built the APIs to make minimal assumptions to allow for the flexibility to support a wide ...\",\"url\":\"https://weave-docs.wandb.ai/tutorial-eval/\"},{\"content\":\"Weave by Weights \\\\u0026 Biases Weave is a toolkit for developing Generative AI applications, built by Weights \\\\u0026 Biases. You can use Weave to:\",\"url\":\"https://github.com/wandb/weave\"}]'})] \n",
      "\n",
      "start=158 end=238 text='capabilities for straightforward evaluation and debugging of GenAI applications.' sources=[ToolSource(type='tool', id='search_internet_k07xfwrwwbz1:0', tool_output={'documents': '[{\"content\":\"Introduction | W\\\\u0026B Weave Open App Trace LLMs Trace Applications App versioning Build an Evaluation Evaluate a RAG App LLM Application Tracing Models Evaluations LLM Providers Local Models On this page Weave is a lightweight toolkit for tracking and evaluating LLM applications, built by Weights \\\\u0026 Biases. Get started by decorating Python functions with @weave.op(). You can use Weave to: Log and debug language model inputs, outputs, and traces Build rigorous, apples-to-apples evaluations for language model use cases Organize all the information generated across the LLM workflow, from experimentation to evaluations to production Try the Quickstart to see Weave in action. Edit this page Next Trace LLMs Weave Weave by W\\\\u0026B\",\"url\":\"https://weave-docs.wandb.ai/\"},{\"content\":\"The solution to this challenge is to treat the model as a closed system where only the inputs and outputs are visible and follow a scientific workflow. This experimental workflow is similar to the workflow machine learning practitioners use to build these LLMs in the first place. W\\\\u0026B Weave was built to enable this experimental workflow:\",\"url\":\"https://wandb.ai/site/press-release/weave-announcement\"},{\"content\":\"Registry Publish and share your ML models and datasets Tables Visualize and explore your ML data Develop GenAI applications with confidence using W\\\\u0026B Weave W\\\\u0026B Weave is here to help developers build and iterate on their AI applications with confidence. Compare different evaluations of model results against different dimensions of performance to ensure applications are as robust as possible when deploying to production. Weave automatically captures all input and output data and builds a tree to help you understand how data flows through your application. Weave is designed with the developer experience in mind, providing capabilities for straightforward evaluation and debugging of GenAI applications. Track and visualize your ML experiments Publish and share your ML models and datasets ML model registry ML model registry\",\"url\":\"https://wandb.ai/site/weave/\"},{\"content\":\"Tutorial: Build an Evaluation pipeline To iterate on an application, we need a way to evaluate if it\\'s improving. To do so, a common practice is to test it against the same set of examples when there is a change. Weave has a first-class way to track evaluations with Model \\\\u0026 Evaluation classes. We have built the APIs to make minimal assumptions to allow for the flexibility to support a wide ...\",\"url\":\"https://weave-docs.wandb.ai/tutorial-eval/\"},{\"content\":\"Weave by Weights \\\\u0026 Biases Weave is a toolkit for developing Generative AI applications, built by Weights \\\\u0026 Biases. You can use Weave to:\",\"url\":\"https://github.com/wandb/weave\"}]'})] \n",
      "\n",
      "start=264 end=320 text='Log and debug language model inputs, outputs, and traces' sources=[ToolSource(type='tool', id='search_internet_k07xfwrwwbz1:0', tool_output={'documents': '[{\"content\":\"Introduction | W\\\\u0026B Weave Open App Trace LLMs Trace Applications App versioning Build an Evaluation Evaluate a RAG App LLM Application Tracing Models Evaluations LLM Providers Local Models On this page Weave is a lightweight toolkit for tracking and evaluating LLM applications, built by Weights \\\\u0026 Biases. Get started by decorating Python functions with @weave.op(). You can use Weave to: Log and debug language model inputs, outputs, and traces Build rigorous, apples-to-apples evaluations for language model use cases Organize all the information generated across the LLM workflow, from experimentation to evaluations to production Try the Quickstart to see Weave in action. Edit this page Next Trace LLMs Weave Weave by W\\\\u0026B\",\"url\":\"https://weave-docs.wandb.ai/\"},{\"content\":\"The solution to this challenge is to treat the model as a closed system where only the inputs and outputs are visible and follow a scientific workflow. This experimental workflow is similar to the workflow machine learning practitioners use to build these LLMs in the first place. W\\\\u0026B Weave was built to enable this experimental workflow:\",\"url\":\"https://wandb.ai/site/press-release/weave-announcement\"},{\"content\":\"Registry Publish and share your ML models and datasets Tables Visualize and explore your ML data Develop GenAI applications with confidence using W\\\\u0026B Weave W\\\\u0026B Weave is here to help developers build and iterate on their AI applications with confidence. Compare different evaluations of model results against different dimensions of performance to ensure applications are as robust as possible when deploying to production. Weave automatically captures all input and output data and builds a tree to help you understand how data flows through your application. Weave is designed with the developer experience in mind, providing capabilities for straightforward evaluation and debugging of GenAI applications. Track and visualize your ML experiments Publish and share your ML models and datasets ML model registry ML model registry\",\"url\":\"https://wandb.ai/site/weave/\"},{\"content\":\"Tutorial: Build an Evaluation pipeline To iterate on an application, we need a way to evaluate if it\\'s improving. To do so, a common practice is to test it against the same set of examples when there is a change. Weave has a first-class way to track evaluations with Model \\\\u0026 Evaluation classes. We have built the APIs to make minimal assumptions to allow for the flexibility to support a wide ...\",\"url\":\"https://weave-docs.wandb.ai/tutorial-eval/\"},{\"content\":\"Weave by Weights \\\\u0026 Biases Weave is a toolkit for developing Generative AI applications, built by Weights \\\\u0026 Biases. You can use Weave to:\",\"url\":\"https://github.com/wandb/weave\"}]'})] \n",
      "\n",
      "start=323 end=396 text='Build rigorous, apples-to-apples evaluations for language model use cases' sources=[ToolSource(type='tool', id='search_internet_k07xfwrwwbz1:0', tool_output={'documents': '[{\"content\":\"Introduction | W\\\\u0026B Weave Open App Trace LLMs Trace Applications App versioning Build an Evaluation Evaluate a RAG App LLM Application Tracing Models Evaluations LLM Providers Local Models On this page Weave is a lightweight toolkit for tracking and evaluating LLM applications, built by Weights \\\\u0026 Biases. Get started by decorating Python functions with @weave.op(). You can use Weave to: Log and debug language model inputs, outputs, and traces Build rigorous, apples-to-apples evaluations for language model use cases Organize all the information generated across the LLM workflow, from experimentation to evaluations to production Try the Quickstart to see Weave in action. Edit this page Next Trace LLMs Weave Weave by W\\\\u0026B\",\"url\":\"https://weave-docs.wandb.ai/\"},{\"content\":\"The solution to this challenge is to treat the model as a closed system where only the inputs and outputs are visible and follow a scientific workflow. This experimental workflow is similar to the workflow machine learning practitioners use to build these LLMs in the first place. W\\\\u0026B Weave was built to enable this experimental workflow:\",\"url\":\"https://wandb.ai/site/press-release/weave-announcement\"},{\"content\":\"Registry Publish and share your ML models and datasets Tables Visualize and explore your ML data Develop GenAI applications with confidence using W\\\\u0026B Weave W\\\\u0026B Weave is here to help developers build and iterate on their AI applications with confidence. Compare different evaluations of model results against different dimensions of performance to ensure applications are as robust as possible when deploying to production. Weave automatically captures all input and output data and builds a tree to help you understand how data flows through your application. Weave is designed with the developer experience in mind, providing capabilities for straightforward evaluation and debugging of GenAI applications. Track and visualize your ML experiments Publish and share your ML models and datasets ML model registry ML model registry\",\"url\":\"https://wandb.ai/site/weave/\"},{\"content\":\"Tutorial: Build an Evaluation pipeline To iterate on an application, we need a way to evaluate if it\\'s improving. To do so, a common practice is to test it against the same set of examples when there is a change. Weave has a first-class way to track evaluations with Model \\\\u0026 Evaluation classes. We have built the APIs to make minimal assumptions to allow for the flexibility to support a wide ...\",\"url\":\"https://weave-docs.wandb.ai/tutorial-eval/\"},{\"content\":\"Weave by Weights \\\\u0026 Biases Weave is a toolkit for developing Generative AI applications, built by Weights \\\\u0026 Biases. You can use Weave to:\",\"url\":\"https://github.com/wandb/weave\"}]'})] \n",
      "\n",
      "start=399 end=512 text='Organize all the information generated across the LLM workflow, from experimentation to evaluations to production' sources=[ToolSource(type='tool', id='search_internet_k07xfwrwwbz1:0', tool_output={'documents': '[{\"content\":\"Introduction | W\\\\u0026B Weave Open App Trace LLMs Trace Applications App versioning Build an Evaluation Evaluate a RAG App LLM Application Tracing Models Evaluations LLM Providers Local Models On this page Weave is a lightweight toolkit for tracking and evaluating LLM applications, built by Weights \\\\u0026 Biases. Get started by decorating Python functions with @weave.op(). You can use Weave to: Log and debug language model inputs, outputs, and traces Build rigorous, apples-to-apples evaluations for language model use cases Organize all the information generated across the LLM workflow, from experimentation to evaluations to production Try the Quickstart to see Weave in action. Edit this page Next Trace LLMs Weave Weave by W\\\\u0026B\",\"url\":\"https://weave-docs.wandb.ai/\"},{\"content\":\"The solution to this challenge is to treat the model as a closed system where only the inputs and outputs are visible and follow a scientific workflow. This experimental workflow is similar to the workflow machine learning practitioners use to build these LLMs in the first place. W\\\\u0026B Weave was built to enable this experimental workflow:\",\"url\":\"https://wandb.ai/site/press-release/weave-announcement\"},{\"content\":\"Registry Publish and share your ML models and datasets Tables Visualize and explore your ML data Develop GenAI applications with confidence using W\\\\u0026B Weave W\\\\u0026B Weave is here to help developers build and iterate on their AI applications with confidence. Compare different evaluations of model results against different dimensions of performance to ensure applications are as robust as possible when deploying to production. Weave automatically captures all input and output data and builds a tree to help you understand how data flows through your application. Weave is designed with the developer experience in mind, providing capabilities for straightforward evaluation and debugging of GenAI applications. Track and visualize your ML experiments Publish and share your ML models and datasets ML model registry ML model registry\",\"url\":\"https://wandb.ai/site/weave/\"},{\"content\":\"Tutorial: Build an Evaluation pipeline To iterate on an application, we need a way to evaluate if it\\'s improving. To do so, a common practice is to test it against the same set of examples when there is a change. Weave has a first-class way to track evaluations with Model \\\\u0026 Evaluation classes. We have built the APIs to make minimal assumptions to allow for the flexibility to support a wide ...\",\"url\":\"https://weave-docs.wandb.ai/tutorial-eval/\"},{\"content\":\"Weave by Weights \\\\u0026 Biases Weave is a toolkit for developing Generative AI applications, built by Weights \\\\u0026 Biases. You can use Weave to:\",\"url\":\"https://github.com/wandb/weave\"}]'})] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"What is Wandb's weave solution?\", search_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: Structured queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tool use setup can be leveraged to perform structured queries. For data sources that contain rich metadata, structured queries can be used to perform highly-specific queries, returning more accurate results.\n",
    "\n",
    "In this example, we can take advantage of metadata available in the code examples dataset such as the file type and language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Any jupyter notebook for Data Versioning with Artifacts?\n",
      "==================================================\n",
      "Tool plan:\n",
      "I will search for a Jupyter notebook for Data Versioning with Artifacts. \n",
      "\n",
      "Tool calls:\n",
      "Tool name: search_code_examples | Parameters: {\"file_type\":\"ipynb\",\"language\":\"en\",\"query\":\"Data Versioning with Artifacts\"}\n",
      "==================================================\n",
      "Response:\n",
      "Yes, there is a Jupyter notebook for Data Versioning with Artifacts (PyTorch).\n",
      "==================================================\n",
      "\n",
      "CITATIONS:\n",
      "start=16 end=77 text='Jupyter notebook for Data Versioning with Artifacts (PyTorch)' sources=[ToolSource(type='tool', id='search_code_examples_f4m0g1kqnedp:0', tool_output={'code_examples': '[{\"content\":\"Interactive W\\\\u0026B Charts Inside Jupyter\",\"file_type\":\"ipynb\",\"language\":\"en\"},{\"content\":\"Model/Data Versioning with Artifacts (PyTorch)\",\"file_type\":\"ipynb\",\"language\":\"en\"},{\"content\":\"Create a hyperparameter search with W\\\\u0026B PyTorch integration\",\"file_type\":\"ipynb\",\"language\":\"en\"},{\"content\":\"Get started with W\\\\u0026B Weave\",\"file_type\":\"ipynb\",\"language\":\"en\"}]'})] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"Any jupyter notebook for Data Versioning with Artifacts?\", search_tools)\n",
    "# Tool call: Searches search_code_examples with file_type = ipynb\n",
    "# Answer: Returns file - Model/Data Versioning with Artifacts (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Structured data queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent can generate queries against structured data sources, such as a CSV file or a database.\n",
    "\n",
    "In this example, we'll use a mock dataset containing LLM application evaluation results for different use cases and settings. Since it's a CSV file, we can create the `analyze_evaluation_results` tool to perform queries on the dataset using the pandas library, executed by a Python interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What's the average evaluation score in run A\n",
      "==================================================\n",
      "Tool plan:\n",
      "I will use the 'analyze_evaluation_results' tool to find the average evaluation score in run A. \n",
      "\n",
      "Tool calls:\n",
      "Tool name: analyze_evaluation_results\n",
      "  import pandas as pd\n",
      "  \n",
      "  df = pd.read_csv(\"evaluation_results.csv\")\n",
      "  \n",
      "  # Filter the dataframe to only include rows where the run is \"A\"\n",
      "  filtered_df = df[df[\"run\"] == \"A\"]\n",
      "  \n",
      "  # Calculate the average score for run A\n",
      "  average_score = filtered_df[\"score\"].mean()\n",
      "  \n",
      "  print(f\"Average score for run A: {average_score}\")\n",
      "None\n",
      "==================================================\n",
      "Response:\n",
      "The average evaluation score in run A is **0.63**.\n",
      "==================================================\n",
      "\n",
      "CITATIONS:\n",
      "start=43 end=47 text='0.63' sources=[ToolSource(type='tool', id='analyze_evaluation_results_7pay84nc1943:0', tool_output={'python_answer': 'Average score for run A: 0.6333333333333334\\n'})] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"What's the average evaluation score in run A\", analysis_tool)\n",
    "# Answer: 0.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What's the latency of the highest-scoring run for the summarize_article use case?\n",
      "==================================================\n",
      "Tool plan:\n",
      "I will use the 'analyze_evaluation_results' tool to find the latency of the highest-scoring run for the summarize_article use case. \n",
      "\n",
      "Tool calls:\n",
      "Tool name: analyze_evaluation_results\n",
      "  import pandas as pd\n",
      "  \n",
      "  df = pd.read_csv(\"evaluation_results.csv\")\n",
      "  \n",
      "  # Filter for the summarize_article use case\n",
      "  filtered_df = df[df[\"usecase\"] == \"summarize_article\"]\n",
      "  \n",
      "  # Find the highest-scoring run\n",
      "  highest_score_run = filtered_df.loc[filtered_df[\"score\"].idxmax()]\n",
      "  \n",
      "  # Get the latency of the highest-scoring run\n",
      "  latency = highest_score_run[\"latency\"]\n",
      "  \n",
      "  print(f\"The latency of the highest-scoring run for the summarize_article use case is {latency} seconds.\")\n",
      "None\n",
      "==================================================\n",
      "Response:\n",
      "The latency of the highest-scoring run for the summarize_article use case is 4.8 seconds.\n",
      "==================================================\n",
      "\n",
      "CITATIONS:\n",
      "start=77 end=89 text='4.8 seconds.' sources=[ToolSource(type='tool', id='analyze_evaluation_results_0jqgr5x3e8nb:0', tool_output={'python_answer': 'The latency of the highest-scoring run for the summarize_article use case is 4.8 seconds.\\n'})] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"What's the latency of the highest-scoring run for the summarize_article use case?\", analysis_tool)\n",
    "# Answer: 4.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Which use case uses the least amount of tokens on average and what's the average token count?\n",
      "==================================================\n",
      "Tool plan:\n",
      "I will use the `analyze_evaluation_results` tool to find out which use case uses the least amount of tokens on average and what the average token count is. \n",
      "\n",
      "Tool calls:\n",
      "Tool name: analyze_evaluation_results\n",
      "  import pandas as pd\n",
      "  \n",
      "  df = pd.read_csv(\"evaluation_results.csv\")\n",
      "  \n",
      "  # Calculate the average number of tokens for each use case\n",
      "  average_tokens_per_usecase = df.groupby(\"usecase\")[\"tokens\"].mean()\n",
      "  \n",
      "  # Find the use case with the lowest average number of tokens\n",
      "  min_tokens_usecase = average_tokens_per_usecase.idxmin()\n",
      "  min_tokens_usecase_avg = average_tokens_per_usecase.min()\n",
      "  \n",
      "  print(f\"Use case with the lowest average number of tokens: {min_tokens_usecase}\")\n",
      "  print(f\"Average number of tokens for that use case: {min_tokens_usecase_avg}\")\n",
      "None\n",
      "==================================================\n",
      "Response:\n",
      "The use case with the lowest average number of tokens is 'extract_names', with an average of 106.25 tokens.\n",
      "==================================================\n",
      "\n",
      "CITATIONS:\n",
      "start=57 end=72 text=\"'extract_names'\" sources=[ToolSource(type='tool', id='analyze_evaluation_results_ygq2yv0fknd6:0', tool_output={'python_answer': 'Use case with the lowest average number of tokens: extract_names\\nAverage number of tokens for that use case: 106.25\\n'})] \n",
      "\n",
      "start=93 end=99 text='106.25' sources=[ToolSource(type='tool', id='analyze_evaluation_results_ygq2yv0fknd6:0', tool_output={'python_answer': 'Use case with the lowest average number of tokens: extract_names\\nAverage number of tokens for that use case: 106.25\\n'})] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"Which use case uses the least amount of tokens on average and what's the average token count?\", analysis_tool)\n",
    "# Answer: extract_names (106.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook demonstrates how we can implement an agentic RAG system with tool use.\n",
    "\n",
    "We covered the following use cases:\n",
    "1. Tool routing\n",
    "2. Parallel tool use\n",
    "3. Multi-step tool use\n",
    "4. Self-correction\n",
    "5. Structured queries\n",
    "6. Structured data queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
